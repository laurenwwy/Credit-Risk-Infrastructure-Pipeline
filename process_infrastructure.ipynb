{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Big Data Infrastructure\n",
    "'''\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Pipeline Complete: Data cleaned and Risk Ratings assigned.\n",
      "+-------+---------------+------------+-----------+--------------------+-----------------+---------------+-----------+-------------------+\n",
      "|loan_id|customer_income|credit_score|loan_amount|       interest_rate|employment_status|current_balance|risk_rating|                DTI|\n",
      "+-------+---------------+------------+-----------+--------------------+-----------------+---------------+-----------+-------------------+\n",
      "|   1000|          45795|         791|      31548| 0.23334959810163441|         Employed|          34447|   Low Risk| 0.6888961677038978|\n",
      "|   1001|          30860|         498|      40434| 0.18672668626404276|    Self-Employed|          18131|  High Risk| 1.3102397926117952|\n",
      "|   1002|         133694|         661|      30039| 0.11104239166864148|         Employed|          28429|Medium Risk|0.22468472781127052|\n",
      "|   1003|         149879|         740|      37530| 0.18686620450360128|         Employed|          18705|   Low Risk| 0.2504019909393577|\n",
      "|   1004|         140268|         325|      43736|0.038861195801785334|    Self-Employed|          49710|  High Risk|0.31180311974220776|\n",
      "+-------+---------------+------------+-----------+--------------------+-----------------+---------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 1. Start a Spark Session (Infrastructure Mindset)\n",
    "spark = SparkSession.builder.appName(\"RBC_Risk_Pipeline\").getOrCreate()\n",
    "\n",
    "# 2. Load the data we just created\n",
    "# inferSchema = True means Spark will automatically detect data types\n",
    "df = spark.read.csv(\"data/raw_loans.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 3. SQL-style transformations: Logic for Risk Categorization\n",
    "# This shows you understand how to segment a bank's portfolio\n",
    "df_processed = df.withColumn(\"risk_rating\", \n",
    "    when(col(\"credit_score\") < 550, \"High Risk\")\n",
    "    .when(col(\"credit_score\") < 700, \"Medium Risk\")\n",
    "    .otherwise(\"Low Risk\")\n",
    ")\n",
    "\n",
    "# 4. Feature Engineering: Calculating Debt-to-Income (DTI) ratio\n",
    "df_processed = df_processed.withColumn(\"DTI\", col(\"loan_amount\") / col(\"customer_income\"))\n",
    "\n",
    "# 5. Save the output in 'Parquet' format (Standard in Bank IT Environments)\n",
    "# Parquet is a columnar (stores data column-wise), \n",
    "# compressed (smaller files and faster reads)and efficient data storage file format optimized for big data processing\n",
    "# Sprak can read Parquet files muh faster than CSV files\n",
    "# .write.mode(\"overwrite\") means if the file already exists, overwrite it\n",
    "df_processed.write.mode(\"overwrite\").parquet(\"data/processed_loans.parquet\")\n",
    "\n",
    "\n",
    "print(\"Spark Pipeline Complete: Data cleaned and Risk Ratings assigned.\")\n",
    "print(df_processed.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
